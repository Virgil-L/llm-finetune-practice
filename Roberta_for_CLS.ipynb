{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 使用RoBERTa-wwm-ext进行文本分类任务","metadata":{"id":"gn7Gqij9DBnO"}},{"cell_type":"markdown","source":"## 1. 准备工作","metadata":{}},{"cell_type":"markdown","source":"### 1.1 环境准备\n","metadata":{"id":"oVFoZq4c4gqo"}},{"cell_type":"code","source":"#关闭安装的输出\n# %%capture\n# !pip install transformers\n# !pip install datasets\n# !pip install evaluate\n# !pip install git+https://github.com/JunnYu/WoBERT_pytorch.git\n","metadata":{"id":"p8RMz9YVAVoX","execution":{"iopub.status.busy":"2023-04-21T06:08:51.104603Z","iopub.execute_input":"2023-04-21T06:08:51.105088Z","iopub.status.idle":"2023-04-21T06:08:51.137647Z","shell.execute_reply.started":"2023-04-21T06:08:51.105035Z","shell.execute_reply":"2023-04-21T06:08:51.136526Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 函数库与gpu使用","metadata":{"id":"zojDZMAt1_db"}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW,SGD,lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom transformers import BertModel, BertTokenizer, BertConfig\nfrom transformers import TrainingArguments, Trainer\nfrom datasets import load_metric\nfrom transformers.trainer_utils import EvalPrediction\nimport torchtext\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n#import evaluate\n\n\nTRAIN_BATCH_SIZE = 20\nVAL_BATCH_SIZE = 20\nTEST_BATCH_SIZE = 20\n\n# 使用gpu进行训练\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n# gpu型号\n!nvidia-smi\n","metadata":{"id":"wan-g7-A3S_f","outputId":"5ef3f7ac-f2ae-44ba-aca9-84467bcc3363","execution":{"iopub.status.busy":"2023-04-21T06:08:51.140029Z","iopub.execute_input":"2023-04-21T06:08:51.140777Z","iopub.status.idle":"2023-04-21T06:09:04.958029Z","shell.execute_reply.started":"2023-04-21T06:08:51.140737Z","shell.execute_reply":"2023-04-21T06:09:04.956716Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using cuda device\nFri Apr 21 06:09:04 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 2. 数据预处理","metadata":{"id":"L8yj_QwJ4vdE"}},{"cell_type":"markdown","source":"### 2.1 数据集导入","metadata":{"id":"iAue3vw-DwXX"}},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, filepath):\n        news_df = pd.read_csv(filepath, encoding='utf-8')\n        texts = news_df.loc[:,'text'].values\n        labels = news_df.loc[:,'class'].values\n    \n        le = LabelEncoder()\n    \n        self.X = texts\n        self.y = le.fit_transform(labels)\n    def __len__(self):\n        return len(self.y)\n  \n    def __getitem__(self, i):\n        text = self.X[i]\n        label = self.y[i]\n        return text, label\n\n\n# train_df = pd.read_csv('/kaggle/input/thucnews-subset-dataset/pr_train.csv',encoding='utf-8')\n# val_df = pd.read_csv('/kaggle/input/thucnews-subset-dataset/pr_val.csv',encoding='utf-8')\n# test_df = pd.read_csv('/kaggle/input/thucnews-subset-dataset/pr_test.csv',encoding='utf-8')\n# train_df.head()\n\ntrain_data = Dataset('/kaggle/input/thucnews-subset-dataset/pr_train.csv')\nval_data = Dataset('/kaggle/input/thucnews-subset-dataset/pr_val.csv')\ntest_data = Dataset('/kaggle/input/thucnews-subset-dataset/pr_test.csv')\ntrain_data.__getitem__(0)","metadata":{"id":"5sb_AGGZ1Iiy","outputId":"b0a9f39f-5de7-490c-ba3e-793a0321707b","execution":{"iopub.status.busy":"2023-04-21T06:09:04.960392Z","iopub.execute_input":"2023-04-21T06:09:04.961883Z","iopub.status.idle":"2023-04-21T06:09:07.485662Z","shell.execute_reply.started":"2023-04-21T06:09:04.961830Z","shell.execute_reply":"2023-04-21T06:09:07.484611Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('马晓旭 意外 受伤 国奥 警惕 无奈 大雨 格外 青睐 殷家 军 记者 傅亚雨 沈阳 报道 来到 沈阳 国奥队 依然 摆脱 雨水 困扰 月 日 下午 国奥队 日常 训练 再度 大雨 干扰 无奈 之下 队员 慢跑 分钟 草草收场 日 上午 国奥队 奥体中心 外场 训练 阴沉沉 气象预报 显示 当天 下午 沈阳 大雨 幸好 队伍 上午 训练 干扰 下午 点当 球队 抵达 训练场 大雨 几个 小时 丝毫 停下来 抱 试一试 态度 球队 当天 下午 例行 训练 分钟 天气 转好 迹象 保护 球员 国奥队 中止 当天 训练 全队 返回 酒店 雨 训练 足球队 稀罕 奥运会 即将 全队 变得 娇贵 沈阳 一周 训练 国奥队 保证 现有 球员 不再 出现意外 伤病 情况 影响 正式 比赛 这一 阶段 控制 训练 受伤 控制 感冒 疾病 队伍 放在 位置 抵达 沈阳 后卫 冯萧霆 训练 冯萧霆 月 日 长春 患上 感冒 参加 日 塞尔维亚 热身赛 队伍 介绍 冯萧霆 发烧 症状 两天 静养 休息 感冒 恢复 训练 冯萧霆 例子 国奥队 对雨中 训练 显得 谨慎 担心 球员 受凉 引发 感冒 非战斗 减员 女足 队员 马晓旭 热身赛 受伤 导致 无缘 奥运 前科 沈阳 国奥队 格外 警惕 训练 嘱咐 队员 动作 再出 事情 一位 工作人员 长春 沈阳 雨水 一路 伴随 国奥队 邪 走 雨 长春 几次 训练 都 大雨 搅和 没想到 沈阳 碰到 事情 一位 国奥 球员 雨水 青睐 不解',\n 0)"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.2 分词和编码","metadata":{"id":"7pVUZsVQDVBr"}},{"cell_type":"code","source":"\ntokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/hflchineserobertawwmext\")\nout = tokenizer.encode(\n    text=train_data.__getitem__(0)[0],\n\n    #当句子长度大于max_length时,截断\n    truncation=True,\n    #一律补pad到max_length长度\n    padding='max_length',\n    add_special_tokens=True,\n    max_length=512,\n    return_tensors=None,\n    is_split_into_words=True\n)\nprint(out)\ntokenizer.decode(out)\n\n#获取字典\n#token_dict = tokenizer.get_vocab()\n#type(token_dict), len(token_dict), '月光' in token_dict\n#添加新词\n#tokenizer.add_token(new_tokens=['月光','希望'])\n#添加新符号\n#tokenizer.add_special_tokens({'eos_token':'[EOS]'})","metadata":{"id":"rJpCYEQJAxX8","outputId":"a54be155-b09b-4d6a-f1a0-a0aec202c238","execution":{"iopub.status.busy":"2023-04-21T06:09:07.488446Z","iopub.execute_input":"2023-04-21T06:09:07.489172Z","iopub.status.idle":"2023-04-21T06:09:07.577739Z","shell.execute_reply.started":"2023-04-21T06:09:07.489141Z","shell.execute_reply":"2023-04-21T06:09:07.576635Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \nThe class this function is called from is 'BertTokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"[101, 7716, 3236, 3195, 2692, 1912, 1358, 839, 1744, 1952, 6356, 2664, 3187, 1937, 1920, 7433, 3419, 1912, 7471, 4712, 3668, 2157, 1092, 6381, 5442, 987, 762, 7433, 3755, 7345, 2845, 6887, 3341, 1168, 3755, 7345, 1744, 1952, 7339, 898, 4197, 3030, 5564, 7433, 3717, 1737, 2817, 3299, 3189, 678, 1286, 1744, 1952, 7339, 3189, 2382, 6378, 5298, 1086, 2428, 1920, 7433, 2397, 2817, 3187, 1937, 722, 678, 7339, 1447, 2714, 6651, 1146, 7164, 5770, 5770, 3119, 1767, 3189, 677, 1286, 1744, 1952, 7339, 1952, 860, 704, 2552, 1912, 1767, 6378, 5298, 7346, 3756, 3756, 3698, 6496, 7564, 2845, 3227, 4850, 2496, 1921, 678, 1286, 3755, 7345, 1920, 7433, 2401, 1962, 7339, 824, 677, 1286, 6378, 5298, 2397, 2817, 678, 1286, 4157, 2496, 4413, 7339, 2850, 6809, 6378, 5298, 1767, 1920, 7433, 1126, 702, 2207, 3198, 692, 3690, 977, 678, 3341, 2849, 6407, 671, 6407, 2578, 2428, 4413, 7339, 2496, 1921, 678, 1286, 891, 6121, 6378, 5298, 1146, 7164, 1921, 3698, 6760, 1962, 6839, 6496, 924, 2844, 4413, 1447, 1744, 1952, 7339, 704, 3632, 2496, 1921, 6378, 5298, 1059, 7339, 6819, 1726, 6983, 2421, 7433, 6378, 5298, 6639, 4413, 7339, 4921, 5383, 1952, 6817, 833, 1315, 2199, 1059, 7339, 1359, 2533, 2019, 6586, 3755, 7345, 671, 1453, 6378, 5298, 1744, 1952, 7339, 924, 6395, 4385, 3300, 4413, 1447, 679, 1086, 1139, 4385, 2692, 1912, 839, 4567, 2658, 1105, 2512, 1510, 3633, 2466, 3683, 6612, 6821, 671, 7348, 3667, 2971, 1169, 6378, 5298, 1358, 839, 2971, 1169, 2697, 1088, 4565, 4567, 7339, 824, 3123, 1762, 855, 5390, 2850, 6809, 3755, 7345, 1400, 1310, 1101, 5854, 7447, 6378, 5298, 1101, 5854, 7447, 3299, 3189, 7270, 3217, 2642, 677, 2697, 1088, 1346, 1217, 3189, 1853, 2209, 5335, 762, 4178, 6716, 6612, 7339, 824, 792, 5305, 1101, 5854, 7447, 1355, 4173, 4568, 4307, 697, 1921, 7474, 1075, 828, 2622, 2697, 1088, 2612, 1908, 6378, 5298, 1101, 5854, 7447, 891, 2094, 1744, 1952, 7339, 2190, 7433, 704, 6378, 5298, 3227, 2533, 6474, 2708, 2857, 2552, 4413, 1447, 1358, 1117, 2471, 1355, 2697, 1088, 7478, 2773, 3159, 1121, 1447, 1957, 6639, 7339, 1447, 7716, 3236, 3195, 4178, 6716, 6612, 1358, 839, 2193, 5636, 3187, 5357, 1952, 6817, 1184, 4906, 3755, 7345, 1744, 1952, 7339, 3419, 1912, 6356, 2664, 6378, 5298, 1671, 1472, 7339, 1447, 1220, 868, 1086, 1139, 752, 2658, 671, 855, 2339, 868, 782, 1447, 7270, 3217, 3755, 7345, 7433, 3717, 671, 6662, 845, 7390, 1744, 1952, 7339, 6932, 6624, 7433, 7270, 3217, 1126, 3613, 6378, 5298, 6963, 1920, 7433, 3009, 1469, 3766, 2682, 1168, 3755, 7345, 4821, 1168, 752, 2658, 671, 855, 1744, 1952, 4413, 1447, 7433, 3717, 7471, 4712, 679, 6237, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'[CLS] 马 晓 旭 意 外 受 伤 国 奥 警 惕 无 奈 大 雨 格 外 青 睐 殷 家 军 记 者 傅 亚 雨 沈 阳 报 道 来 到 沈 阳 国 奥 队 依 然 摆 脱 雨 水 困 扰 月 日 下 午 国 奥 队 日 常 训 练 再 度 大 雨 干 扰 无 奈 之 下 队 员 慢 跑 分 钟 草 草 收 场 日 上 午 国 奥 队 奥 体 中 心 外 场 训 练 阴 沉 沉 气 象 预 报 显 示 当 天 下 午 沈 阳 大 雨 幸 好 队 伍 上 午 训 练 干 扰 下 午 点 当 球 队 抵 达 训 练 场 大 雨 几 个 小 时 丝 毫 停 下 来 抱 试 一 试 态 度 球 队 当 天 下 午 例 行 训 练 分 钟 天 气 转 好 迹 象 保 护 球 员 国 奥 队 中 止 当 天 训 练 全 队 返 回 酒 店 雨 训 练 足 球 队 稀 罕 奥 运 会 即 将 全 队 变 得 娇 贵 沈 阳 一 周 训 练 国 奥 队 保 证 现 有 球 员 不 再 出 现 意 外 伤 病 情 况 影 响 正 式 比 赛 这 一 阶 段 控 制 训 练 受 伤 控 制 感 冒 疾 病 队 伍 放 在 位 置 抵 达 沈 阳 后 卫 冯 萧 霆 训 练 冯 萧 霆 月 日 长 春 患 上 感 冒 参 加 日 塞 尔 维 亚 热 身 赛 队 伍 介 绍 冯 萧 霆 发 烧 症 状 两 天 静 养 休 息 感 冒 恢 复 训 练 冯 萧 霆 例 子 国 奥 队 对 雨 中 训 练 显 得 谨 慎 担 心 球 员 受 凉 引 发 感 冒 非 战 斗 减 员 女 足 队 员 马 晓 旭 热 身 赛 受 伤 导 致 无 缘 奥 运 前 科 沈 阳 国 奥 队 格 外 警 惕 训 练 嘱 咐 队 员 动 作 再 出 事 情 一 位 工 作 人 员 长 春 沈 阳 雨 水 一 路 伴 随 国 奥 队 邪 走 雨 长 春 几 次 训 练 都 大 雨 搅 和 没 想 到 沈 阳 碰 到 事 情 一 位 国 奥 球 员 雨 水 青 睐 不 解 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"},"metadata":{}}]},{"cell_type":"markdown","source":"### 2.3 定义批处理函数","metadata":{"id":"IvSQLaDQ5nWs"}},{"cell_type":"code","source":"def collate_fn(data):\n    sents = [i[0] for i in data]\n    labels = [i[1] for i in data]\n  #编码\n    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n                                 truncation=True,\n                                 padding='max_length',\n                                 max_length=512,\n                                 return_tensors='pt',\n                                 return_length=True,\n                                       is_split_into_words=True,)\n  #input_ids:编码之后的数字\n  #attention_mask:补零的位置三0，其他位置是1\n    input_ids = data['input_ids'].to(device)\n    attention_mask = data['attention_mask'].to(device)\n    token_type_ids = data['token_type_ids'].to(device)\n    labels = torch.LongTensor(labels).to(device)\n\n    return input_ids, attention_mask, token_type_ids, labels\n\n\n#导入数据，这一步合并到trainner中了\n\ntrain_loader = DataLoader(dataset=train_data,\n                    batch_size=TRAIN_BATCH_SIZE,\n                    collate_fn=collate_fn,\n                    shuffle=True,\n                    drop_last=True)\n\nval_loader = DataLoader(dataset=val_data,\n                           batch_size=VAL_BATCH_SIZE,\n                           collate_fn=collate_fn,\n                           shuffle=True,\n                           drop_last=True)\n\ntest_loader = DataLoader(dataset=test_data,\n                           batch_size=TEST_BATCH_SIZE,\n                           collate_fn=collate_fn,\n                           shuffle=True,\n                           drop_last=True)\n\n# for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n#     break\n# print(len(train_loader))\n# input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape\n\n\n# for batch in train_dataloader:\n#   batch = {k: v.to(device) for k, v in batch.items()}\n#   outputs = model(**batch)","metadata":{"id":"BG0IpD_J5qW4","outputId":"88eae1ca-5f49-4ef3-a9e9-0d9752ac219f","execution":{"iopub.status.busy":"2023-04-21T06:09:07.579540Z","iopub.execute_input":"2023-04-21T06:09:07.580276Z","iopub.status.idle":"2023-04-21T06:09:07.590675Z","shell.execute_reply.started":"2023-04-21T06:09:07.580233Z","shell.execute_reply":"2023-04-21T06:09:07.589621Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## 加载预训练模型","metadata":{"id":"w18MfAc9EZ6J"}},{"cell_type":"code","source":"#加载预训练模型\n#CN_roberta_wwm_ext = BertModel.from_pretrained(\"/kaggle/input/hflchineserobertawwmext\").to(device)\n# #不使用finetune时直接冻结预训练模型的参数\n# for param in wobert_pretrained.parameters():\n#     param.requires_grad_(False)\n\n#模型试算\n#out = bert_bc_pretrained(input_ids=input_ids,\n#                 attention_mask=attention_mask,\n#                 token_type_ids=token_type_ids)\n\n#out.last_hidden_state.shape\n\n#模型参数查看\n#print('param_num: ' + str(sum([i.nelement() for i in CN_roberta_wwm_ext.parameters()]) / 10000))","metadata":{"id":"2FS9Y1woEXRK","outputId":"ff1591df-7974-4a31-8227-491618e52978","execution":{"iopub.status.busy":"2023-04-21T06:09:07.593501Z","iopub.execute_input":"2023-04-21T06:09:07.594752Z","iopub.status.idle":"2023-04-21T06:09:07.605252Z","shell.execute_reply.started":"2023-04-21T06:09:07.594714Z","shell.execute_reply":"2023-04-21T06:09:07.604185Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## 定义下游任务模型","metadata":{"id":"LkNE7SM1xg2i"}},{"cell_type":"code","source":"# class Downstream_Model(nn.Module):\n#     def __init__(self, ):\n#         super().__init__()\n#         self.fc = nn.Linear(768,10)\n  \n#     def forward(self, input_ids, attention_mask, token_type_ids):\n#         out = CN_roberta_wwm_ext(input_ids=input_ids,\n#                  attention_mask=attention_mask,\n#                  token_type_ids=token_type_ids)\n      \n#         out = self.fc(out.last_hidden_state[:, 0])\n#         out = out.softmax(dim=1)\n\n#         return out\n\n\nclass Classifier(nn.Module):\n    # 加个全连接 进行分类\n    def __init__(self, num_cls):\n        super(Classifier, self).__init__()\n        self.dense1 = torch.nn.Linear(768, 192)\n        self.dense2 = torch.nn.Linear(192, num_cls)\n        self.activation = torch.nn.Tanh()\n        self.dropout = torch.nn.Dropout(0.1)\n\n    def forward(self, x):\n        x = self.dropout(x)\n        x = self.dense1(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.dense2(x)\n        return x\n\n\nclass RoBerta_CLS_Model(nn.Module):\n    def __init__(self, label_num):\n        super(RoBerta_CLS_Model, self).__init__()\n        self.config = BertConfig.from_pretrained('/kaggle/input/hflchineserobertawwmext/config.json')\n        self.config.output_hidden_states = True   # 输出所有的隐层\n        self.config.output_attentions = True  # 输出所有注意力层计算结果\n        self.roberta = BertModel.from_pretrained('/kaggle/input/hflchineserobertawwmext', config=self.config)\n\n        num_cls = label_num\n        # self.highway = Highway(size=768, num_layers=3)\n        self.classifier = Classifier(num_cls)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output = self.roberta(input_ids=input_ids, \n                              attention_mask=attention_mask, \n                              token_type_ids=token_type_ids)\n        # output[0].size(): batch_size, max_len, hidden_size\n        # output[1].size(): batch_size, hidden_size\n        # len(output[2]): 13, 其中一个元素的尺寸: batch_size, max_len, hidden_size\n        # len(output[3]): 12, 其中一个元素的尺寸: batch_size, 12层, max_len, max_len\n\n        cls_output = output[1]\n        # hw_output = self.highway(cls_output)\n        logits = self.classifier(cls_output)\n        return logits\n\nmodel = RoBerta_CLS_Model(label_num=10).to(device)\nprint('param_num: ' + str(sum([i.nelement() for i in model.parameters()]) / 10000))","metadata":{"id":"rox3vdQmxj1f","execution":{"iopub.status.busy":"2023-04-21T06:09:07.608726Z","iopub.execute_input":"2023-04-21T06:09:07.609032Z","iopub.status.idle":"2023-04-21T06:09:16.973836Z","shell.execute_reply.started":"2023-04-21T06:09:07.609005Z","shell.execute_reply":"2023-04-21T06:09:16.972620Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\nSome weights of the model checkpoint at /kaggle/input/hflchineserobertawwmext were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"param_num: 10241.7226\n","output_type":"stream"}]},{"cell_type":"code","source":"# for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n#     break\n\n# with torch.no_grad(): \n#     out = model(input_ids=input_ids,\n#                  attention_mask=attention_mask,\n#                  token_type_ids=token_type_ids)\n\n# out","metadata":{"execution":{"iopub.status.busy":"2023-04-21T06:09:16.975546Z","iopub.execute_input":"2023-04-21T06:09:16.975925Z","iopub.status.idle":"2023-04-21T06:09:16.980831Z","shell.execute_reply.started":"2023-04-21T06:09:16.975887Z","shell.execute_reply":"2023-04-21T06:09:16.979729Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## 4. 训练下游任务模型","metadata":{"id":"dztSnKfh0K43"}},{"cell_type":"code","source":"def train_func(model, output_model_dir, loss_fn, n_epochs, train_loader,scheduler ,var_loader=None):\n    for t in range(n_epochs):\n        print(f\"Epoch {t+1}\\n-------------------------------\")\n\n        model.train()\n        size = len(train_loader.dataset)\n        for batch, (input_ids, attention_mask, token_type_ids, labels) in enumerate(train_loader):\n            out = model(input_ids=input_ids,\n                  attention_mask=attention_mask,\n                  token_type_ids=token_type_ids)\n\n            loss = loss_fn(out, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            #以batch为最小检查单位，进行学习率衰减，patience=500\n            scheduler.step()\n\n            if batch % 250 == 0:\n                loss, current = loss.item(), (batch+1) * len(input_ids)\n                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n        if var_loader != None:\n            test_func(dataloader=val_loader, model=model, loss_fn=loss_fn, tp = 'validation')\n            \n#         if t==round(n_epochS/2) or t==n_epochs-1:\n#             output_model_file = os.path.join(output_model_dir, \"finetuned_model_epoch_{}.bin\".format(t+1))\n#             torch.save(model_to_save.state_dict(), output_model_file)\n#             print(\"\\n model_saved\")\n    print(\"\\n training_finished\")\n    output_model_file = os.path.join(output_model_dir, \"finetuned_model_epoch_{}.bin\".format(n_epochs))\n    torch.save(model.state_dict(), output_model_file)\n    \n    \n\ndef test_func(dataloader, model, loss_fn, tp='validation'):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with torch.no_grad(): #停止梯度计算\n        for i, (input_ids, attention_mask, token_type_ids, labels) in enumerate(dataloader):\n            pred = model(input_ids=input_ids,\n                         attention_mask=attention_mask,\n                         token_type_ids=token_type_ids)\n            test_loss += loss_fn(pred, labels).item()\n            correct += (pred.argmax(1) == labels).type(torch.float).sum().item()\n  \n        test_loss /= num_batches\n        correct /= size\n\n    if tp == 'test':\n        print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n    elif tp == 'validation':\n        print(f\"Val Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n","metadata":{"id":"MR2lfgyZ0JV8","execution":{"iopub.status.busy":"2023-04-21T06:09:16.982411Z","iopub.execute_input":"2023-04-21T06:09:16.983068Z","iopub.status.idle":"2023-04-21T06:09:17.010064Z","shell.execute_reply.started":"2023-04-21T06:09:16.983031Z","shell.execute_reply":"2023-04-21T06:09:17.005767Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import pathlib\npathlib.Path('/kaggle/working/output_model').mkdir(parents=True, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T06:09:17.017605Z","iopub.execute_input":"2023-04-21T06:09:17.018572Z","iopub.status.idle":"2023-04-21T06:09:17.026386Z","shell.execute_reply.started":"2023-04-21T06:09:17.018524Z","shell.execute_reply":"2023-04-21T06:09:17.024957Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"INITIAL_LR = 0.01\noptimizer = SGD(model.parameters(),lr = INITIAL_LR)\n#scheduler = lr_scheduler.StepLR(optimizer,step_size=5,gamma = 0.5)\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=500, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=1, min_lr=1e-8, eps=1e-8)\n\ntrain_func(train_loader = train_loader,\n           var_loader= val_loader, \n           model = model, \n           output_model_dir = '/kaggle/working/output_model',\n           loss_fn = nn.CrossEntropyLoss(), \n           optimizer = SGD(model.parameters(), lr=0.01), \n           n_epochs=10)","metadata":{"id":"yyEjTnU3R67u","outputId":"581bd534-2f31-40a3-aab2-7a8d5cf30683","execution":{"iopub.status.busy":"2023-04-21T07:15:19.944045Z","iopub.execute_input":"2023-04-21T07:15:19.944576Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1\n-------------------------------\nloss: 2.194166  [   20/50000]\nloss: 0.308747  [ 4020/50000]\nloss: 0.062948  [ 8020/50000]\nloss: 0.082081  [12020/50000]\nloss: 0.030117  [16020/50000]\nloss: 0.327527  [20020/50000]\nloss: 0.061981  [24020/50000]\nloss: 0.014001  [28020/50000]\nloss: 0.016874  [32020/50000]\n","output_type":"stream"}]},{"cell_type":"code","source":"# #定义优化器、损失函数、评价指标\n# optimizer = AdamW(model.parameters(), lr=5e-4)\n# loss_fn = nn.CrossEntropyLoss()\n# metric = evaluate.load('accuracy')\n\n# #初始化训练参数\n# args = TrainingArguments(output_dir=None,\n#                          overwrite_output_dir = False,\n#                          evaluation_strategy='epoch',\n#                          num_train_epochs = 20,\n#                          learning_rate = 1e-4, #优化器默认为AdamW\n#                          adam_beta1 = 0.9,\n#                          adam_beta2 = 0.999,\n#                          adam_epsilon = 1e-8,\n#                          weight_decay = 1e-2, #各层的权重衰减\n#                          max_grad_norm = 1.0, #梯度裁剪\n#                          per_device_eval_batch_size = 128,\n#                          per_device_train_batch_size = 128,\n#                          lr_scheduler_type = 'linear',\n#                          save_strategy = 'epoch',\n#                          no_cuda = False,\n#                          seed = 1024,\n#                          data_seed = 1024,\n#                          load_best_model_at_end = False,\n#                          metric_for_best_model = 'loss',\n#                          greater_is_better = False\n#                          )\n\n# class CustomTrainer(Trainer): #可能需要自己继承trainer,覆写一些函数\n#     def get_train_dataloader():\n#         return\n#     def get_test_dataloader():\n#         return\n\n# #初始化训练器                         \n# trainer = Trainer(\n#     model = model,\n#     args = args,\n#     data_collator = collate_fn, #构建batch\n#     train_dataset = train_data,\n#     eval_dataset = val_data,\n#     compute_metrics = metric,\n#     tokenizer = tokenizer\n#     #callbacks = \n#     #optimizers = \n# )","metadata":{"execution":{"iopub.status.busy":"2023-04-21T07:03:35.771931Z","iopub.status.idle":"2023-04-21T07:03:35.773721Z","shell.execute_reply.started":"2023-04-21T07:03:35.773453Z","shell.execute_reply":"2023-04-21T07:03:35.773482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. 测试模型效果","metadata":{"id":"JJPAHGr-LEW8"}},{"cell_type":"code","source":"test_func(dataloader=test_loader, \n          model=model, \n          loss_fn = nn.CrossEntropyLoss(), \n          tp='test')","metadata":{"id":"H8ZyuCD9K_tN","execution":{"iopub.status.busy":"2023-04-21T07:03:35.775310Z","iopub.status.idle":"2023-04-21T07:03:35.776138Z","shell.execute_reply.started":"2023-04-21T07:03:35.775874Z","shell.execute_reply":"2023-04-21T07:03:35.775902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6.保存模型","metadata":{}},{"cell_type":"code","source":"model.state_dict()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T07:03:35.777646Z","iopub.status.idle":"2023-04-21T07:03:35.778494Z","shell.execute_reply.started":"2023-04-21T07:03:35.778189Z","shell.execute_reply":"2023-04-21T07:03:35.778216Z"},"trusted":true},"execution_count":null,"outputs":[]}]}